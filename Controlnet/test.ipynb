{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "import os\n",
    "import PIL\n",
    "from metrics import LPIPSMetric, SSIM , PSNR, F1_score\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "remote_sensing_model = \"tjisousa/sd-remote-sensing-model-256\"\n",
    "\n",
    "controlnet_path = \"Saved Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    remote_sensing_model, controlnet=controlnet, torch_dtype=torch.float16,\n",
    "    cache_dir=\"RS_Saved Model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "# remove following line if xformers is not installed or when using Torch 2.0.\n",
    "# pipe.enable_xformers_memory_efficient_attention()\n",
    "# memory optimization.\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Please just run the following code once you bring back the original ControlNet stable diffusion Model. \n",
    "'''\n",
    "\n",
    "image_directory= \"F:\\Shahmir\\ControlNet Satellite Imagery\\Masks\"\n",
    "image_files = os.listdir(image_directory)\n",
    "output_directory= \"F:\\Shahmir\\ControlNet Satellite Imagery\\Vanilla Test Images\"\n",
    "gt_directory = \"F:\\Shahmir\\ControlNet Satellite Imagery\\Ground Truths\"\n",
    "gt_files= os.listdir(gt_directory)\n",
    "out_directory= \"F:\\Shahmir\\ControlNet Satellite Imagery\\Test\"\n",
    "# output_directory= \"F:\\Shahmir\\ControlNet Satellite Imagery\\RSI Test Images\"\n",
    "# gt_directory=\"F:\\Shahmir\\ControlNet Satellite Imagery\\Vanilla Test Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in image_files:\n",
    "    image_path = os.path.join(image_directory, image)\n",
    "    print(image)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in image_files:\n",
    "    image_path = os.path.join(image_directory, image)\n",
    "    control_image = load_image(image_path)\n",
    "    # prompt =\" Generate a realistic high- resolution satellite image of a which is zoomed out city with very little vegetation.\\\n",
    "    # # Houses should have brown roofs.\\\n",
    "    # # Focus on surrounding vegetation which is deep green with varied shades but majorly contains barren landscape. \\\n",
    "    # # Roads should be sharply defined against the landscape. Image should be ultra high defination.\"\n",
    "    prompt =\"A greener image. Segmentation area should have construction only\"\n",
    "    generator = torch.manual_seed(0)\n",
    "    output = pipe(prompt, num_inference_steps=100, generator=generator, image=control_image).images[0]\n",
    "    output_path = os.path.join(out_directory, image)\n",
    "    print(output_path)\n",
    "    #control_image.save(output_path)\n",
    "\n",
    "    output.save(output_path)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lpips_list = []\n",
    "ssim_list = []\n",
    "psnr_list = []\n",
    "for index, image in enumerate(image_files):\n",
    " \n",
    "\n",
    "    Lpips_metric = LPIPSMetric()\n",
    "    ssim_metric = SSIM()\n",
    "    psnr_metric = PSNR()\n",
    "\n",
    "  \n",
    "    \n",
    "    image_path = os.path.join(image_directory, image)\n",
    "    control_image = load_image(image_path)\n",
    "    prompt =\" Generate a high- resolution  aerial satellite image of a city with lots of trees and brown landscape \\\n",
    "    Houses should have brown roofs.\\\n",
    "    Focus on surrounding vegetation which is deep green with varied shades. \\\n",
    "    Roads should be sharply defined agaisnt the landscape. Image should be ultra high defination.\"\n",
    "    gt_path = os.path.join(gt_directory, gt_files[index])\n",
    "\n",
    "    generator = torch.manual_seed(0)\n",
    "    output = pipe(prompt, num_inference_steps=100, generator=generator, image=control_image).images[0]\n",
    "    output_path = os.path.join(output_directory, image)\n",
    "    output.save(output_path)\n",
    "    \n",
    "    output=data_transform(output)\n",
    "    if output.shape != (3, 256, 256):\n",
    "        output= output[:3, :256, :256]    \n",
    "        \n",
    "    gt=Image.open(gt_path)\n",
    "    gt= data_transform(gt)\n",
    "    gt= gt[:3, :, :]\n",
    "    \n",
    "    Lpips_metric.update(output.round().detach().cpu(),gt.detach().cpu())\n",
    "    ssim_metric.update(output.unsqueeze(dim=0).round().detach().cpu(), gt.unsqueeze(dim=0).detach().cpu())\n",
    "    psnr_metric.update(output.round().detach().cpu(), gt.detach().cpu())\n",
    "\n",
    "    Lpips_list.append(Lpips_metric.compute())\n",
    "    ssim_list.append(ssim_metric.compute())\n",
    "    psnr_list.append(psnr_metric.compute()) \n",
    "\n",
    "    print(f\"At index f{index},LPIPS: {Lpips_metric.compute()}, SSIM: {ssim_metric.compute()}, PSNR: {psnr_metric.compute()}\")\n",
    "\n",
    "avg_lpip = np.mean(Lpips_list)\n",
    "avg_ssim = np.mean(ssim_list)   \n",
    "avg_psnr = np.mean(psnr_list)\n",
    "\n",
    "print(f\"Average LPIPS: {avg_lpip}, Average SSIM: {avg_ssim}, Average PSNR: {avg_psnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LPIPS: {np.mean(Lpips_list)} SSIM: {np.mean(ssim_list)} PSNR: {np.mean(psnr_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt, num_inference_steps=100, generator=generator, image=control_image\n",
    ").images[0]\n",
    "image.save(\"/output.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
